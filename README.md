# 🚀 XIK的AI实验室

<div align="center">
  <h3>专注于大模型与检索增强生成(RAG)的研究与应用</h3>
</div>

<p align="center">
  <img src="https://img.shields.io/badge/LLM-专家-blue?style=for-the-badge" />
  <img src="https://img.shields.io/badge/RAG-研究者-green?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Transformer-工程师-orange?style=for-the-badge" />
</p>

## 🧠 Transformer架构

<div align="center">
<img src="./images/transformer.png" alt="Attention Is All You Need论文首页" width="40%" />
<p><i>👆 "Attention Is All You Need" - Transformer架构奠基论文</i></p>
</div>

<details>
<summary><b>🔍 点击展开Transformer详细结构说明</b></summary>

### Transformer架构核心组件

1. **输入嵌入 (Input Embedding)**：将输入token转换为向量表示
2. **位置编码 (Positional Encoding)**：为序列中的每个位置添加位置信息
3. **多头注意力机制 (Multi-Head Attention)**：
   - 查询(Query)、键(Key)、值(Value)三个映射矩阵
   - 并行计算多个注意力"头"，捕获不同角度的信息
4. **前馈神经网络 (Feed Forward Network)**：由两个线性变换和ReLU激活函数组成
5. **Add & Norm**：残差连接和层归一化，保证训练稳定性
6. **编码器-解码器结构**：用于序列到序列任务

</details>

## 🔥 核心技术栈

```mermaid
graph TD
    A[大语言模型] --> B[Transformer架构优化]
    A --> C[RAG技术]
    B --> D[注意力机制改进]
    B --> E[长文本处理]
    C --> F[知识图谱增强]
    C --> G[语义检索]
    C --> H[上下文工程]
    I[应用开发] --> J[Vue+FastAPI]
    I --> K[轻量级部署]
    L[领域应用] --> M[智能问答]
    L --> N[知识管理]
```


## 📊 数据可视化

<div align="center">
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=Xikcn&layout=compact&theme=radical" width="45%" />
<img src="https://github-readme-streak-stats.herokuapp.com/?user=Xikcn&theme=radical" width="45%" />
</div>

## 🔗 了解更多

<p align="center">
  <a href="https://github.com/Xikcn"><img src="https://img.shields.io/badge/GitHub-Xikcn-100000?style=for-the-badge&logo=github&logoColor=white" /></a>
</p>

---

> "AI不是为了替代人类，而是为了增强人类能力，拓展我们的认知边界。"
